{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML : Linear Regression Example1\n",
    "\n",
    "\n",
    "### Concepts :\n",
    "\n",
    "* Creating RDD using SparkContext\n",
    "* Providing schema to create a DataFrame from an RDD\n",
    "* Performing basic data analysis using Spark SQL\n",
    "* Using Spark ML to perform train a linear regression model\n",
    "\n",
    "### Input Dataset :\n",
    "\n",
    "* California Housing Dataset, housing prices per 'blocks' of census. Each row in the dataset corresponds to a block group. A block corresponds to a group of citizens that live in a geographically compact area\n",
    "\n",
    "### Objective :\n",
    "\n",
    "* Build a model that is able to predict the median house price\n",
    "\n",
    "### Dataset Details:\n",
    "\n",
    "Features : \n",
    "\n",
    "* Latitude \n",
    "* Longitude\n",
    "* Housing median age : median age of the people that belong to a block group \n",
    "* Total rooms : total nb of rooms in the houses of the block group \n",
    "* Total bedrooms : total nb of bedrooms in the houses of the block group\n",
    "* Population : nb of inhabitants of a block group \n",
    "* Households : units of houses and their occupants per block group \n",
    "* Median income : median income of people that belong to a block group \n",
    "\n",
    "Target :\n",
    "\n",
    "* Median house value \n",
    "\n",
    "### Overall Workflow\n",
    "\n",
    "1. Load Data\n",
    "2. Inspect Data\n",
    "3. Preprocess Data\n",
    "4. Create Model\n",
    "5. Make Predictions\n",
    "6. Evaluate how good are our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "my_home=os.environ['HOME']\n",
    "dataset_path=my_home+\"/spark-course/data/housing_data/\"\n",
    "outputs_path=my_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/spark2-client\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession and specify configuration\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Lab5-ML-LinearRegression-Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1.2.6.2.14-5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Direct data inspection shows that the input data has no header.\n",
    "There are several ways this can be tacked , I provide here only 2 examples on how to provide the schema, to construc the data frame\n",
    "  * option 1 : use the Row object,construct a DataFrame by creating Row objects (remember a DataFrame is a Dataset[Row]\n",
    "  * option 2 : infer the schema from the data and add a header\n",
    "  * option 3 : manually provide a schema using the StructType construct\n",
    "  * there are possible other options , even simpler ... provide it yourself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLine(line):\n",
    "    \"\"\" Parse a line from the input data\n",
    "    Args:\n",
    "        line (str): a line (row) of the input data file\n",
    "    Returns:\n",
    "        Row : row object containin the parsed elements from the line\n",
    "        Note we are adding schema by directly transforming the str into double types\n",
    "    \"\"\"\n",
    "    \n",
    "    parts=re.split(\",\", line)\n",
    "   \n",
    "    # Read in each feature PLUS THE TARGET\n",
    "    lat = parts[0]\n",
    "    lon = parts[1]\n",
    "    age = parts[2]\n",
    "    trm = parts[3]\n",
    "    tbr = parts[4]\n",
    "    pop = parts[5]\n",
    "    hou = parts[6]\n",
    "    inc = parts[7]\n",
    "    val = parts[8]\n",
    "        \n",
    "    return Row(\n",
    "                latitude=float(lat),\n",
    "                longitude=float(lon),\n",
    "                median_housing_age=float(age),\n",
    "                total_rooms=float(trm),\n",
    "                total_bedrooms=float(tbr),\n",
    "                population=float(pop),\n",
    "                households=float(hou),\n",
    "                median_income=float(inc),\n",
    "                median_value=float(val)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Option 1 : use SparkContext and a function to map each line to a Row object\n",
    "# ---------\n",
    "from pyspark.sql import Row\n",
    "import re\n",
    "sc=spark.sparkContext\n",
    "rdd = sc.textFile(\"file://\"+dataset_path+\"*.data\")\n",
    "#\n",
    "df = rdd \\\n",
    "        .map(lambda line: readLine(line)) \\\n",
    "        .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+------------------+-------------+------------+----------+--------------+-----------+\n",
      "|households|latitude|longitude|median_housing_age|median_income|median_value|population|total_bedrooms|total_rooms|\n",
      "+----------+--------+---------+------------------+-------------+------------+----------+--------------+-----------+\n",
      "|     126.0| -122.23|    37.88|              41.0|       8.3252|    452600.0|     322.0|         129.0|      880.0|\n",
      "|    1138.0| -122.22|    37.86|              21.0|       8.3014|    358500.0|    2401.0|        1106.0|     7099.0|\n",
      "|     177.0| -122.24|    37.85|              52.0|       7.2574|    352100.0|     496.0|         190.0|     1467.0|\n",
      "|     219.0| -122.25|    37.85|              52.0|       5.6431|    341300.0|     558.0|         235.0|     1274.0|\n",
      "|     259.0| -122.25|    37.85|              52.0|       3.8462|    342200.0|     565.0|         280.0|     1627.0|\n",
      "|     193.0| -122.25|    37.85|              52.0|       4.0368|    269700.0|     413.0|         213.0|      919.0|\n",
      "|     514.0| -122.25|    37.84|              52.0|       3.6591|    299200.0|    1094.0|         489.0|     2535.0|\n",
      "|     647.0| -122.25|    37.84|              52.0|         3.12|    241400.0|    1157.0|         687.0|     3104.0|\n",
      "|     595.0| -122.26|    37.84|              42.0|       2.0804|    226700.0|    1206.0|         665.0|     2555.0|\n",
      "|     714.0| -122.25|    37.84|              52.0|       3.6912|    261100.0|    1551.0|         707.0|     3549.0|\n",
      "+----------+--------+---------+------------------+-------------+------------+----------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- median_housing_age: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_value: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Option 2 :  use SparkSession and infer schema, then add a header\n",
    "# ---------\n",
    "\n",
    "df2 = spark.read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"*.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: double (nullable = true)\n",
      " |-- _c4: double (nullable = true)\n",
      " |-- _c5: double (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- median_housing_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "features=[ \"latitude\",\"longitude\",\"median_housing_age\", \\\n",
    "            \"total_rooms\",\"total_bedrooms\",\"population\", \\\n",
    "            \"households\",\"median_income\"]\n",
    "target=[\"median_value\"]\n",
    "\n",
    "fieldnames=features+target\n",
    "\n",
    "rawnames=df2.schema.names\n",
    "\n",
    "# Create a small function\n",
    "def updateColNames(df,oldnames,newnames):\n",
    "    for i in range(len(newnames)):\n",
    "        df=df.withColumnRenamed(oldnames[i], newnames[i])\n",
    "    return df\n",
    "\n",
    "df2=updateColNames(df2,rawnames,fieldnames)\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Option 3 :  manually provide a Schema\n",
    "# ---------\n",
    "\n",
    "# from pyspark.sql.types import *\n",
    "# fieldnames=[ \"latitude\",\"longitude\",\"median_housing_age\", \\\n",
    "#            \"total_rooms\",\"total_bedrooms\",\"population\", \\\n",
    "#            \"households\",\"median_income\",\"median_value\"]\n",
    "# def applySchema(x,fieldnames):\n",
    "#      fields = [StructField(field_name, DoubleType(), True) for field_name in fieldnames]\n",
    "#      schema = StructType(fields)\n",
    "#      return x\n",
    "# \n",
    "# features = rdd \\\n",
    "#            .map(lambda line: line.split(\",\")) \\\n",
    "#            .map(lambda x : applySchema(x,fieldnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table for SQL access\n",
    "df.registerTempTable(\"houses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Records\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "|summary|       total_rooms|   total_bedrooms|     median_income|        population|\n",
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "|  count|             20640|            20640|             20640|             20640|\n",
      "|   mean|2635.7630813953488|537.8980135658915|3.8706710029070246|1425.4767441860465|\n",
      "| stddev|2181.6152515827944| 421.247905943133| 1.899821717945263|  1132.46212176534|\n",
      "|    min|               2.0|              1.0|            0.4999|               3.0|\n",
      "|    max|           39320.0|           6445.0|           15.0001|           35682.0|\n",
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics on the selected (or full) set of fields of the dataframe\n",
    "# ( Remember the total rooms are PER GROPU BLOCK of census , not per house ..., obviously)\n",
    "df.select('total_rooms','total_bedrooms','median_income','population').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worth Noting Here\n",
    "\n",
    "See here that the **standard deviation is in almost all cases of the order of the mean value**\n",
    "\n",
    "**Meaning there is a large spread in our data** and pointing to the fact that we will need to somehow **normalize our data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small function\n",
    "# that computes the correlation of each column against the target\n",
    "# Computes Pearson Correlation Coefficient between the two columns\n",
    "def computeCorrelation(df,targetColumnName):\n",
    "    for col in df.columns:\n",
    "        r=df.stat.corr(col,targetColumnName)\n",
    "        print(\"Pearson correlation : %s %s %f \\n\" %(col,targetColumnName,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation : households median_value 0.065843 \n",
      "\n",
      "Pearson correlation : latitude median_value -0.045967 \n",
      "\n",
      "Pearson correlation : longitude median_value -0.144160 \n",
      "\n",
      "Pearson correlation : median_housing_age median_value 0.105623 \n",
      "\n",
      "Pearson correlation : median_income median_value 0.688075 \n",
      "\n",
      "Pearson correlation : median_value median_value 1.000000 \n",
      "\n",
      "Pearson correlation : population median_value -0.024650 \n",
      "\n",
      "Pearson correlation : total_bedrooms median_value 0.050594 \n",
      "\n",
      "Pearson correlation : total_rooms median_value 0.134153 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# THERE SEEMS TO BE A SPARK INSTALLATION PROBLEM WITH TEH STATS LIB\n",
    "# FIXME !\n",
    "computeCorrelation(df, 'median_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "\n",
    "You would typically take a sub sample (no replacement here) from the data JUST for plotting purposes.\n",
    "\n",
    "Sampling with or without replacement has important statistical differences ( selection bias ), but we are jsut plotting\n",
    "\n",
    "Usefull explanation of sample with or without replacement implications here:\n",
    "\n",
    "https://www.ma.utexas.edu/users/parker/sampling/repl.htm\n",
    "\n",
    "Linear Relationships (plotting):\n",
    "https://seaborn.pydata.org/tutorial/regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.lmplot(\"median_income\", \"median_value\", data=df.toPandas(), fit_reg=True, markers=\".\")\n",
    "#\n",
    "# SIMPLE , least squares fit\n",
    "# \n",
    "# IMPORTANT NOTE : you will -always- want to scale your data\n",
    "# before doing this below , is there just for simple demo purposes\n",
    "#\n",
    "import numpy as np\n",
    "resi,rank,sing,rcond,c=np.polyfit(df.toPandas()['median_income'], df.toPandas()['median_value'], 1,full=True)\n",
    "print('residuals : %f' % resi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", color_codes=True )\n",
    "sns.pairplot(df.toPandas(), markers=\".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "**Adjust our target variable**\n",
    "\n",
    "As we said before is recommended to scale the target variable prior to model creation. Doing this will avoid problems during the model creation and predictions computation, due to large values and possible outliers in the data. This also eases the scaling process later.\n",
    "\n",
    "**Add some 'new' features** to our existing set of features.\n",
    "This are qualitative information that could help us in predicting the median_value of a house\n",
    "\n",
    "1. Rooms per household :  number of rooms in households per block group\n",
    "2. Population per household :  how many people live in households per block group \n",
    "3. Bedrooms per room : gives you an idea about how many rooms are bedrooms per block group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the values of `median_value`\n",
    "# We will express the median_value in units of 100\n",
    "from pyspark.sql.functions import *\n",
    "df = df.withColumn(\"median_value\", col(\"median_value\")/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# 1.\n",
    "roomsPerHousehold = df['total_rooms']/df['households']\n",
    "\n",
    "# 2.\n",
    "populationPerHousehold = df['population']/df['households']\n",
    "\n",
    "# 3.\n",
    "bedroomsPerRoom = df['total_bedrooms']/df['total_rooms']\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\",roomsPerHousehold) \\\n",
    "       .withColumn(\"populationPerHousehold\",populationPerHousehold) \\\n",
    "       .withColumn(\"bedroomsPerRoom\", bedroomsPerRoom)\n",
    "   \n",
    "# Check what is the output\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "df = df.select('median_value',\n",
    "              'total_bedrooms', \n",
    "              'population', \n",
    "              'households', \n",
    "              'median_income', \n",
    "              'roomsPerHousehold', \n",
    "              'populationPerHousehold', \n",
    "              'bedroomsPerRoom'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# We need to transform each row of feature into a vector ( a continous space of values )\n",
    "# for the algorithm : in particular a DenseVector\n",
    "# The density of a vector is defined by the number of empty values it has. \n",
    "# lesser empty values, bigger density of the vector\n",
    "\n",
    "# A full view of data types in RDD based API ;\n",
    "# https://spark.apache.org/docs/2.2.0/mllib-data-types.html\n",
    "\n",
    "\n",
    "# Define the input_data \n",
    "# The median value (row[0]) is our target variable ( the label )\n",
    "# The rest of the values row[1:] our our features\n",
    "data = df.rdd.map(lambda row: (row[0], DenseVector(row[1:])))\n",
    "\n",
    "# Replace df with the new DataFrame\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling ( Standarization )\n",
    "\n",
    "At this stage we can see that features are not scaled.\n",
    "\n",
    "Scaling features is a very common pre-processing step and can improve the convergence rate during the optimization process, and also prevents against features with very large variances exerting an overly large influence during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in df using the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\n",
    "print('Training records : %d' % train_data.count())\n",
    "print('Test records : %d ' % test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the models\n",
    "linearModelA = lr.fit(train_data,{lr.regParam:0.1})\n",
    "linearModelB = lr.fit(train_data,{lr.regParam:0.3})\n",
    "linearModelC = lr.fit(train_data,{lr.regParam:0.6})\n",
    "# Generate predictions for models\n",
    "predictedA = linearModelA.transform(test_data)\n",
    "predictedB = linearModelB.transform(test_data)\n",
    "predictedC = linearModelC.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedB.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction')\n",
    "scoreA = evaluator.evaluate(predictedA)\n",
    "scoreB = evaluator.evaluate(predictedB)\n",
    "scoreC = evaluator.evaluate(predictedC)\n",
    "print('Score for model A is : %f' % scoreA )\n",
    "print('Score for model B is : %f' % scoreB )\n",
    "print('Score for model C is : %f' % scoreC )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RMSE ( standard deviation of the residuals ) residual = predicted - observed\n",
    "# It indicates the absolute fit of the model to the data ,\n",
    "# or how close the observed data points are to the model's predicted values\n",
    "# The smaller an RMSE value, the closer predicted and observed values are.\n",
    "linearModelA.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the R2\n",
    "# The R2 (coefficient of determination) is a measure \n",
    "# of the dispersion of the data with respect to fitted regression line, a relative measurement,\n",
    "# and varies between 0-100% \n",
    "# 0% indicates that the model explains none of the variability of the response data around its mean, \n",
    "# and 100% indicates the opposite: it explains all the variability. \n",
    "# In gemneral , the higher the R-squared, the better the model fits your data.\n",
    "linearModelA.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
