{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Course : Assignment2\n",
    "\n",
    "#### Compulsory Part :\n",
    "\n",
    "Implement a Spark application that performs the following operations:\n",
    "￼\n",
    "1. Loads the provided input log file for this assignment without specifying the num of partitions.\n",
    "2. Counts the number occurrences of the word “​info​” in the log\n",
    "3. Counts the number occurrences of the word “​error​” in the log\n",
    "4. Saves -each- of the results from steps b and c into a text file format output (one output file for errors , another for info)\n",
    "\n",
    "From the Spark Web UI answer the following questions:\n",
    "\n",
    "1. How many jobs were created for your Spark App\n",
    "2. How many stages did each job split up into\n",
    "3. How many tasks per stage did your application split up into\n",
    "4. How many RDD blocks are present in your application\n",
    "\n",
    "#### Bonus Part\n",
    "\n",
    "Implement a Spark application that performs the following operations:\n",
    "\n",
    "1. How many distinct ip addresses appear in this log\n",
    "2. How many entries of each distinct ip can be found in the log\n",
    "3. Which is the latest entry in the log for ip = 64.242.88.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compulsory Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compulsory Part : \n",
    "# 1 : Loads the provided input log file for this assignment without specifying the num of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('app.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compulsory Part : \n",
    "# 2 : Counts the number occurrences of the word “info” in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[info]', 96)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : \"info\" in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add)\n",
    "\n",
    "infos.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compulsory Part : \n",
    "# 3 : Counts the number occurrences of the word “error” in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[error]', 5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : \"error\" in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add)\n",
    "\n",
    "errors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compulsory Part :\n",
    "# 4 : Saves -each- of the results from steps b and c into a text file format output (one output file for errors , another for info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that infos and errors are RDD\n",
    "# So the result of this save are the RDDs partition contents\n",
    "# These will be directories with the partitions contents\n",
    "infos.saveAsTextFile('infos_part')\n",
    "errors.saveAsTextFile('errors_part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From the Spark Web UI answer the following questions:\n",
    "# 1. How many jobs were created for your Spark App\n",
    "# 2. How many stages did each job split up into\n",
    "# 3. How many tasks per stage did your application split up into\n",
    "# 4. How many RDD blocks are present in your application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ANSWERS\n",
    "\n",
    "1. A total of 4 Jobs have been created :\n",
    "\n",
    "    - 1 job for the collect action applied to obtain the counts of 'info' occurrences\n",
    "    - 1 job for the collect action applied to obtain the counts of 'error' occurences\n",
    "    - 1 job to save the rdd contents of infos\n",
    "    - 1 job to save the rdd contents of errors\n",
    "\n",
    "2. A total of 6 Stages have been created :  \n",
    "\n",
    "    - Each collect job was split in 2 stages : ( 2 job x 2 stage/job  = 4 ) \n",
    "    \n",
    "      This is due to a shuffle boundary introduced by the reduceByKey transformation\n",
    "    \n",
    "    - Each saveAsTextFile job only had 1 stage : ( 2 job x 1 stage/job   = 2 )\n",
    "    \n",
    "      Note : 1 stage skip due to the same cause. When a shuffle operation is detected\n",
    "      and because it's an expensive operation Spark automatically caches the generated partitions \n",
    "      (totally in heap if they fit) to avoid recomputation from scratch if a new action is then triggered.\n",
    "      \n",
    "      In our case that's exactly what is happening. When we called collect() action Spark cached (silently)\n",
    "      the partitions for us. When we called another action saveAsTextFile() Spark used the cached partitions  \n",
    "      to avoid recomputation. \n",
    "      \n",
    "      See : https://spark.apache.org/docs/1.5.0/programming-guide.html#performance-impact\n",
    "      \n",
    "      \n",
    "3. A total of 12 Tasks have been created :  \n",
    "\n",
    "     - Each stage in generated for a collect job was split in 4 tasks ( 2 stages x 4 task/stage = 8 )\n",
    "     - Each stage in generated for a saveAsTextFile job was split in 2 tasks ( 2 stages x 2 task/stage = 4 )\n",
    "     \n",
    "4. http://localhost:4040/executors/ A total of 1 RDD blocks have been generated\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bonus Part:\n",
    "# 1 : How many distinct ip addresses appear in this log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a 'quick and dirty' way\n",
    "ips = lines.flatMap(lambda x: x.split(\"[\")) \\\n",
    "        .flatMap(lambda x: x.split(\"]\")) \\\n",
    "        .filter(lambda x : \"client\" in x) \\\n",
    "        .filter(lambda x : \".\" in x) \\\n",
    "        .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client 64.242.88.10',\n",
       " 'client 24.70.56.49',\n",
       " 'client 24.71.236.129',\n",
       " 'client 200.174.151.3',\n",
       " 'client 61.9.4.61',\n",
       " 'client 81.226.63.194',\n",
       " 'client 140.113.179.131']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. How many entries of each distinct ip can be found in the log\n",
    "entries = lines.flatMap(lambda x: x.split(\"[\")) \\\n",
    "        .flatMap(lambda x: x.split(\"]\")) \\\n",
    "        .filter(lambda x : \"client\" in x) \\\n",
    "        .filter(lambda x : \".\" in x) \\\n",
    "        .map(lambda x : x.split()) \\\n",
    "        .map(lambda ip : (ip[1],1)) \\\n",
    "        .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('61.9.4.61', 2),\n",
       " ('81.226.63.194', 1),\n",
       " ('200.174.151.3', 1),\n",
       " ('140.113.179.131', 1),\n",
       " ('64.242.88.10', 93),\n",
       " ('24.70.56.49', 1),\n",
       " ('24.71.236.129', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Which is the latest entry in the log for ip = 64.242.88.10\n",
    "\n",
    "# This part requires some non-trivial coding\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def parseTime(s):\n",
    "    \"\"\" Create a Datetime object\n",
    "    \n",
    "    Typically: datetime.datetime(2003, 8, 4, 12, 30, 45)\n",
    "    \n",
    "    Args:\n",
    "        s (str): date and time (example : \"Sun Mar  7 16:02:00 2004\")\n",
    "    Returns:\n",
    "        datetime: datetime object\n",
    "    \"\"\"\n",
    "    time_parts=s.split(\" \")\n",
    "    \n",
    "    months_map = dict(\n",
    "        [(\"Jan\", 1),(\"Feb\", 2),(\"Mar\", 3), \\\n",
    "        (\"Apr\", 4),(\"May\", 5),(\"Jun\", 6), \\\n",
    "        (\"Jul\", 7),(\"Aug\", 8),(\"Sep\", 9), \\\n",
    "        (\"Oct\", 10),(\"Nov\", 11),(\"Dec\", 12)]\n",
    "        )\n",
    "        \n",
    "    return datetime.datetime(int(time_parts[5]),\n",
    "                             int(months_map[time_parts[1]]),\n",
    "                             int(time_parts[3]),\n",
    "                             int(time_parts[4].split(\":\")[0]),\n",
    "                             int(time_parts[4].split(\":\")[1]),\n",
    "                             int(time_parts[4].split(\":\")[2])\n",
    "                            )  \n",
    "\n",
    "def parseLogLine(line):\n",
    "    \"\"\" Parse a line from the log\n",
    "    Args:\n",
    "        logline (str): a line of text in the log\n",
    "        example : \n",
    "        [Sun Mar  7 17:27:37 2004] [info] [client 64.242.88.10] (104)Connection reset by peer: client stopped connection before send body completed\n",
    "        But some lines will not contain all the fields\n",
    "    Returns:\n",
    "        Tuple: \n",
    "          success : containing the parsed elements from the parsing and a 1\n",
    "            error : containing the original line and 0\n",
    "    \"\"\"\n",
    "    \n",
    "    LOG_PATTERN=\"\\[([^]]+)\\]\"\n",
    "         \n",
    "    parts=re.split(LOG_PATTERN, line)\n",
    "   \n",
    "    time = parts[1]\n",
    "    info = parts[3]\n",
    "    rest = parts[5]\n",
    "       \n",
    "    return (\n",
    "            Row(\n",
    "                datetime = parseTime(time),\n",
    "                infofield= info,\n",
    "                ipaddress= rest \n",
    "           ), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = lines.map(lambda line: parseLogLine(line))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
