{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab0 : Spark Word Count\n",
    "\n",
    "### Topics : \n",
    "\n",
    "* RDD Creation\n",
    "* RDD Transformations and Actions\n",
    "\n",
    "### Example objetive :\n",
    "\n",
    "Given an input file , compute the nb of ocurrences of a particular word inside the file\n",
    "\n",
    "### Reference :\n",
    "\n",
    "* SPARK Reference Documentation: https://spark.apache.org/docs/2.3.1/programming-guide.html#rdd-operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile=\"data/server.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an RDD : by loading from a data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 156 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of lines \n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of partitions for this RDD \n",
    "lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the word you want to search for\n",
    "search_word='error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Transformations and actions to compute the result\n",
    "\n",
    "Transformations : \n",
    "    \n",
    "1. flatMap() transformation : split each line into the words that form it , split by whitespace\n",
    "2. filter() transformation:  filter on each line those words that are equal to the search word\n",
    "3. map() transformation : create a tuple with each filtered word on each line and a counter\n",
    "4. reduceByKey() transformation : aggregate based on the keys(=distinct words) with a sum function (add) over all lines\n",
    "\n",
    "Action : \n",
    "    \n",
    "1. collect() : return all elements from the computed RDD\n",
    "\n",
    "Lazy Evaluation :\n",
    "\n",
    "* Until the collect() action is called nothing actually happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_rdd = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : search_word in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Job Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[7] at RDD at PythonRDD.scala:52 []\n",
      " |  MapPartitionsRDD[6] at mapPartitions at PythonRDD.scala:132 []\n",
      " |  ShuffledRDD[5] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[4] at reduceByKey at <ipython-input-7-33f738cb430a>:1 []\n",
      "    |  PythonRDD[3] at reduceByKey at <ipython-input-7-33f738cb430a>:1 []\n",
      "    |  data/app.log MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  data/app.log HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "# See the RDD lineage\n",
    "print(counts_rdd.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lineage is telling us that there will be 2 stages for this spark job\n",
    "# A shuffling of data is involved because the reduceByKey \n",
    "# requires to place all items belonging to the same key on the same partition \n",
    "# shuffling operation marks the boundary between stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = counts_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[error]: 5\n"
     ]
    }
   ],
   "source": [
    "for word, count in errors:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Room for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now , imagine we want to search a set of words ...\n",
    "# Do you want to repeat every time the loading and split by whitespace operations ?\n",
    "# These are going to be repeated every time unless we cache ...\n",
    "cached_lines = lines.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now search for other words\n",
    "search_word='info'\n",
    "counts_rdd = cached_lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : search_word in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos = counts_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info]: 96\n"
     ]
    }
   ],
   "source": [
    "for word, count in infos:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis\n",
    "\n",
    "* In the Spark Web UI Inspect the storage tab.\n",
    "* You should see that the RDD has been cached , saved directly in memory\n",
    "* Now perform again and operation , like count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now search for other words\n",
    "search_word='dummy'\n",
    "counts_rdd = cached_lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .filter(lambda x : search_word in x) \\\n",
    "        .map(lambda word : (word, 1)) \\\n",
    "        .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummys = counts_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
