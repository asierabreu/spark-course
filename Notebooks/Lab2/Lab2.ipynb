{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 : Working with Spark SQL\n",
    "\n",
    "We will review :\n",
    "\n",
    "1. Loading CSV file formats using SparkSession\n",
    "2. Creating DataFrame without inferring Schema \n",
    "3. Creating DataFrame inferring Schema \n",
    "4. Creating DataFrame using databricks library that help inferring the schema\n",
    "5. Creating partitioned parquet files format\n",
    "6. Doing some preliminary analysis of this dataset\n",
    "\n",
    "Dataset :\n",
    "\n",
    "* Air flight data - subset of ~ few GBs ( Include reference )\n",
    "\n",
    "Dataset path (in the cluster) :\n",
    "\n",
    "* /data/shared/spark/flight_data/csv_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Let's start by :\n",
    "# 1. Definining SPARK_HOME variable \n",
    "# 2. Using findspark to  let us work with Spark installation in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME']=\"/usr/hdp/current/spark2-client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-SQL-Lab2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"/data/shared/spark/flight_data/csv_small/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one of the data files into a data frame\n",
    "df = spark.read \\\n",
    "    .csv(\"file://\"+dataset_path+\"On_Time_On_Time_Performance_2014_9.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check the data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      " |-- _c31: string (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      " |-- _c34: string (nullable = true)\n",
      " |-- _c35: string (nullable = true)\n",
      " |-- _c36: string (nullable = true)\n",
      " |-- _c37: string (nullable = true)\n",
      " |-- _c38: string (nullable = true)\n",
      " |-- _c39: string (nullable = true)\n",
      " |-- _c40: string (nullable = true)\n",
      " |-- _c41: string (nullable = true)\n",
      " |-- _c42: string (nullable = true)\n",
      " |-- _c43: string (nullable = true)\n",
      " |-- _c44: string (nullable = true)\n",
      " |-- _c45: string (nullable = true)\n",
      " |-- _c46: string (nullable = true)\n",
      " |-- _c47: string (nullable = true)\n",
      " |-- _c48: string (nullable = true)\n",
      " |-- _c49: string (nullable = true)\n",
      " |-- _c50: string (nullable = true)\n",
      " |-- _c51: string (nullable = true)\n",
      " |-- _c52: string (nullable = true)\n",
      " |-- _c53: string (nullable = true)\n",
      " |-- _c54: string (nullable = true)\n",
      " |-- _c55: string (nullable = true)\n",
      " |-- _c56: string (nullable = true)\n",
      " |-- _c57: string (nullable = true)\n",
      " |-- _c58: string (nullable = true)\n",
      " |-- _c59: string (nullable = true)\n",
      " |-- _c60: string (nullable = true)\n",
      " |-- _c61: string (nullable = true)\n",
      " |-- _c62: string (nullable = true)\n",
      " |-- _c63: string (nullable = true)\n",
      " |-- _c64: string (nullable = true)\n",
      " |-- _c65: string (nullable = true)\n",
      " |-- _c66: string (nullable = true)\n",
      " |-- _c67: string (nullable = true)\n",
      " |-- _c68: string (nullable = true)\n",
      " |-- _c69: string (nullable = true)\n",
      " |-- _c70: string (nullable = true)\n",
      " |-- _c71: string (nullable = true)\n",
      " |-- _c72: string (nullable = true)\n",
      " |-- _c73: string (nullable = true)\n",
      " |-- _c74: string (nullable = true)\n",
      " |-- _c75: string (nullable = true)\n",
      " |-- _c76: string (nullable = true)\n",
      " |-- _c77: string (nullable = true)\n",
      " |-- _c78: string (nullable = true)\n",
      " |-- _c79: string (nullable = true)\n",
      " |-- _c80: string (nullable = true)\n",
      " |-- _c81: string (nullable = true)\n",
      " |-- _c82: string (nullable = true)\n",
      " |-- _c83: string (nullable = true)\n",
      " |-- _c84: string (nullable = true)\n",
      " |-- _c85: string (nullable = true)\n",
      " |-- _c86: string (nullable = true)\n",
      " |-- _c87: string (nullable = true)\n",
      " |-- _c88: string (nullable = true)\n",
      " |-- _c89: string (nullable = true)\n",
      " |-- _c90: string (nullable = true)\n",
      " |-- _c91: string (nullable = true)\n",
      " |-- _c92: string (nullable = true)\n",
      " |-- _c93: string (nullable = true)\n",
      " |-- _c94: string (nullable = true)\n",
      " |-- _c95: string (nullable = true)\n",
      " |-- _c96: string (nullable = true)\n",
      " |-- _c97: string (nullable = true)\n",
      " |-- _c98: string (nullable = true)\n",
      " |-- _c99: string (nullable = true)\n",
      " |-- _c100: string (nullable = true)\n",
      " |-- _c101: string (nullable = true)\n",
      " |-- _c102: string (nullable = true)\n",
      " |-- _c103: string (nullable = true)\n",
      " |-- _c104: string (nullable = true)\n",
      " |-- _c105: string (nullable = true)\n",
      " |-- _c106: string (nullable = true)\n",
      " |-- _c107: string (nullable = true)\n",
      " |-- _c108: string (nullable = true)\n",
      " |-- _c109: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ok , but the column names are not very telling. \n",
    "* How to improve this? , by telling Spark to use the header ( if exists )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"On_Time_On_Time_Performance_2014_9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Quarter: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- DayofMonth: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- AirlineID: string (nullable = true)\n",
      " |-- Carrier: string (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- FlightNum: string (nullable = true)\n",
      " |-- OriginAirportID: string (nullable = true)\n",
      " |-- OriginAirportSeqID: string (nullable = true)\n",
      " |-- OriginCityMarketID: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- OriginState: string (nullable = true)\n",
      " |-- OriginStateFips: string (nullable = true)\n",
      " |-- OriginStateName: string (nullable = true)\n",
      " |-- OriginWac: string (nullable = true)\n",
      " |-- DestAirportID: string (nullable = true)\n",
      " |-- DestAirportSeqID: string (nullable = true)\n",
      " |-- DestCityMarketID: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- DestState: string (nullable = true)\n",
      " |-- DestStateFips: string (nullable = true)\n",
      " |-- DestStateName: string (nullable = true)\n",
      " |-- DestWac: string (nullable = true)\n",
      " |-- CRSDepTime: string (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- DepDelayMinutes: string (nullable = true)\n",
      " |-- DepDel15: string (nullable = true)\n",
      " |-- DepartureDelayGroups: string (nullable = true)\n",
      " |-- DepTimeBlk: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- WheelsOff: string (nullable = true)\n",
      " |-- WheelsOn: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- CRSArrTime: string (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- ArrDelayMinutes: string (nullable = true)\n",
      " |-- ArrDel15: string (nullable = true)\n",
      " |-- ArrivalDelayGroups: string (nullable = true)\n",
      " |-- ArrTimeBlk: string (nullable = true)\n",
      " |-- Cancelled: string (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- Flights: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- DistanceGroup: string (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      " |-- FirstDepTime: string (nullable = true)\n",
      " |-- TotalAddGTime: string (nullable = true)\n",
      " |-- LongestAddGTime: string (nullable = true)\n",
      " |-- DivAirportLandings: string (nullable = true)\n",
      " |-- DivReachedDest: string (nullable = true)\n",
      " |-- DivActualElapsedTime: string (nullable = true)\n",
      " |-- DivArrDelay: string (nullable = true)\n",
      " |-- DivDistance: string (nullable = true)\n",
      " |-- Div1Airport: string (nullable = true)\n",
      " |-- Div1AirportID: string (nullable = true)\n",
      " |-- Div1AirportSeqID: string (nullable = true)\n",
      " |-- Div1WheelsOn: string (nullable = true)\n",
      " |-- Div1TotalGTime: string (nullable = true)\n",
      " |-- Div1LongestGTime: string (nullable = true)\n",
      " |-- Div1WheelsOff: string (nullable = true)\n",
      " |-- Div1TailNum: string (nullable = true)\n",
      " |-- Div2Airport: string (nullable = true)\n",
      " |-- Div2AirportID: string (nullable = true)\n",
      " |-- Div2AirportSeqID: string (nullable = true)\n",
      " |-- Div2WheelsOn: string (nullable = true)\n",
      " |-- Div2TotalGTime: string (nullable = true)\n",
      " |-- Div2LongestGTime: string (nullable = true)\n",
      " |-- Div2WheelsOff: string (nullable = true)\n",
      " |-- Div2TailNum: string (nullable = true)\n",
      " |-- Div3Airport: string (nullable = true)\n",
      " |-- Div3AirportID: string (nullable = true)\n",
      " |-- Div3AirportSeqID: string (nullable = true)\n",
      " |-- Div3WheelsOn: string (nullable = true)\n",
      " |-- Div3TotalGTime: string (nullable = true)\n",
      " |-- Div3LongestGTime: string (nullable = true)\n",
      " |-- Div3WheelsOff: string (nullable = true)\n",
      " |-- Div3TailNum: string (nullable = true)\n",
      " |-- Div4Airport: string (nullable = true)\n",
      " |-- Div4AirportID: string (nullable = true)\n",
      " |-- Div4AirportSeqID: string (nullable = true)\n",
      " |-- Div4WheelsOn: string (nullable = true)\n",
      " |-- Div4TotalGTime: string (nullable = true)\n",
      " |-- Div4LongestGTime: string (nullable = true)\n",
      " |-- Div4WheelsOff: string (nullable = true)\n",
      " |-- Div4TailNum: string (nullable = true)\n",
      " |-- Div5Airport: string (nullable = true)\n",
      " |-- Div5AirportID: string (nullable = true)\n",
      " |-- Div5AirportSeqID: string (nullable = true)\n",
      " |-- Div5WheelsOn: string (nullable = true)\n",
      " |-- Div5TotalGTime: string (nullable = true)\n",
      " |-- Div5LongestGTime: string (nullable = true)\n",
      " |-- Div5WheelsOff: string (nullable = true)\n",
      " |-- Div5TailNum: string (nullable = true)\n",
      " |-- _c109: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Still one caveat though , all values are interpreted as StringType() While some of them (actually most), are of numeric nature ( e.g ) Year , Month , Flight Number\n",
    "* How to improve this ?, by telling Spark to infer the Schema of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"On_Time_On_Time_Performance_2014_9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Quarter: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: timestamp (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- AirlineID: integer (nullable = true)\n",
      " |-- Carrier: string (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- OriginAirportID: integer (nullable = true)\n",
      " |-- OriginAirportSeqID: integer (nullable = true)\n",
      " |-- OriginCityMarketID: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- OriginState: string (nullable = true)\n",
      " |-- OriginStateFips: integer (nullable = true)\n",
      " |-- OriginStateName: string (nullable = true)\n",
      " |-- OriginWac: integer (nullable = true)\n",
      " |-- DestAirportID: integer (nullable = true)\n",
      " |-- DestAirportSeqID: integer (nullable = true)\n",
      " |-- DestCityMarketID: integer (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- DestState: string (nullable = true)\n",
      " |-- DestStateFips: integer (nullable = true)\n",
      " |-- DestStateName: string (nullable = true)\n",
      " |-- DestWac: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepDelayMinutes: double (nullable = true)\n",
      " |-- DepDel15: double (nullable = true)\n",
      " |-- DepartureDelayGroups: integer (nullable = true)\n",
      " |-- DepTimeBlk: string (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- WheelsOff: integer (nullable = true)\n",
      " |-- WheelsOn: integer (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrDelayMinutes: double (nullable = true)\n",
      " |-- ArrDel15: double (nullable = true)\n",
      " |-- ArrivalDelayGroups: integer (nullable = true)\n",
      " |-- ArrTimeBlk: string (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Flights: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- DistanceGroup: integer (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- FirstDepTime: integer (nullable = true)\n",
      " |-- TotalAddGTime: double (nullable = true)\n",
      " |-- LongestAddGTime: double (nullable = true)\n",
      " |-- DivAirportLandings: integer (nullable = true)\n",
      " |-- DivReachedDest: double (nullable = true)\n",
      " |-- DivActualElapsedTime: double (nullable = true)\n",
      " |-- DivArrDelay: double (nullable = true)\n",
      " |-- DivDistance: double (nullable = true)\n",
      " |-- Div1Airport: string (nullable = true)\n",
      " |-- Div1AirportID: integer (nullable = true)\n",
      " |-- Div1AirportSeqID: integer (nullable = true)\n",
      " |-- Div1WheelsOn: integer (nullable = true)\n",
      " |-- Div1TotalGTime: double (nullable = true)\n",
      " |-- Div1LongestGTime: double (nullable = true)\n",
      " |-- Div1WheelsOff: integer (nullable = true)\n",
      " |-- Div1TailNum: string (nullable = true)\n",
      " |-- Div2Airport: string (nullable = true)\n",
      " |-- Div2AirportID: integer (nullable = true)\n",
      " |-- Div2AirportSeqID: integer (nullable = true)\n",
      " |-- Div2WheelsOn: integer (nullable = true)\n",
      " |-- Div2TotalGTime: double (nullable = true)\n",
      " |-- Div2LongestGTime: double (nullable = true)\n",
      " |-- Div2WheelsOff: integer (nullable = true)\n",
      " |-- Div2TailNum: string (nullable = true)\n",
      " |-- Div3Airport: string (nullable = true)\n",
      " |-- Div3AirportID: string (nullable = true)\n",
      " |-- Div3AirportSeqID: string (nullable = true)\n",
      " |-- Div3WheelsOn: string (nullable = true)\n",
      " |-- Div3TotalGTime: string (nullable = true)\n",
      " |-- Div3LongestGTime: string (nullable = true)\n",
      " |-- Div3WheelsOff: string (nullable = true)\n",
      " |-- Div3TailNum: string (nullable = true)\n",
      " |-- Div4Airport: string (nullable = true)\n",
      " |-- Div4AirportID: string (nullable = true)\n",
      " |-- Div4AirportSeqID: string (nullable = true)\n",
      " |-- Div4WheelsOn: string (nullable = true)\n",
      " |-- Div4TotalGTime: string (nullable = true)\n",
      " |-- Div4LongestGTime: string (nullable = true)\n",
      " |-- Div4WheelsOff: string (nullable = true)\n",
      " |-- Div4TailNum: string (nullable = true)\n",
      " |-- Div5Airport: string (nullable = true)\n",
      " |-- Div5AirportID: string (nullable = true)\n",
      " |-- Div5AirportSeqID: string (nullable = true)\n",
      " |-- Div5WheelsOn: string (nullable = true)\n",
      " |-- Div5TotalGTime: string (nullable = true)\n",
      " |-- Div5LongestGTime: string (nullable = true)\n",
      " |-- Div5WheelsOff: string (nullable = true)\n",
      " |-- Div5TailNum: string (nullable = true)\n",
      " |-- _c109: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that a way to have data with schema , let's read in some more data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Quarter: int, Month: int, DayofMonth: int, DayOfWeek: int, FlightDate: timestamp, UniqueCarrier: string, AirlineID: int, Carrier: string, TailNum: string, FlightNum: int, OriginAirportID: int, OriginAirportSeqID: int, OriginCityMarketID: int, Origin: string, OriginCityName: string, OriginState: string, OriginStateFips: int, OriginStateName: string, OriginWac: int, DestAirportID: int, DestAirportSeqID: int, DestCityMarketID: int, Dest: string, DestCityName: string, DestState: string, DestStateFips: int, DestStateName: string, DestWac: int, CRSDepTime: int, DepTime: int, DepDelay: double, DepDelayMinutes: double, DepDel15: double, DepartureDelayGroups: int, DepTimeBlk: string, TaxiOut: double, WheelsOff: int, WheelsOn: int, TaxiIn: double, CRSArrTime: int, ArrTime: int, ArrDelay: double, ArrDelayMinutes: double, ArrDel15: double, ArrivalDelayGroups: int, ArrTimeBlk: string, Cancelled: double, CancellationCode: string, Diverted: double, CRSElapsedTime: double, ActualElapsedTime: double, AirTime: double, Flights: double, Distance: double, DistanceGroup: int, CarrierDelay: double, WeatherDelay: double, NASDelay: double, SecurityDelay: double, LateAircraftDelay: double, FirstDepTime: int, TotalAddGTime: double, LongestAddGTime: double, DivAirportLandings: int, DivReachedDest: double, DivActualElapsedTime: double, DivArrDelay: double, DivDistance: double, Div1Airport: string, Div1AirportID: int, Div1AirportSeqID: int, Div1WheelsOn: int, Div1TotalGTime: double, Div1LongestGTime: double, Div1WheelsOff: int, Div1TailNum: string, Div2Airport: string, Div2AirportID: int, Div2AirportSeqID: int, Div2WheelsOn: int, Div2TotalGTime: double, Div2LongestGTime: double, Div2WheelsOff: int, Div2TailNum: string, Div3Airport: string, Div3AirportID: string, Div3AirportSeqID: string, Div3WheelsOn: string, Div3TotalGTime: string, Div3LongestGTime: string, Div3WheelsOff: string, Div3TailNum: string, Div4Airport: string, Div4AirportID: string, Div4AirportSeqID: string, Div4WheelsOn: string, Div4TotalGTime: string, Div4LongestGTime: string, Div4WheelsOff: string, Div4TailNum: string, Div5Airport: string, Div5AirportID: string, Div5AirportSeqID: string, Div5WheelsOn: string, Div5TotalGTime: string, Div5LongestGTime: string, Div5WheelsOff: string, Div5TailNum: string, _c109: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ IN ONE YEAR WORTH OF DATA : ~ 2.5GB\n",
    "df_small = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"file://\"+dataset_path+\"*csv\") \n",
    "df_small.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save this dataframe in parquet (columnar) format for boost in loading performance\n",
    "* In order to do we want to 'be clever' and partition the data by specific atributes , in this case\n",
    "* Year and Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o237.parquet.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=aabreu, access=WRITE, inode=\"/data/home/aabreu/airline_data/_temporary/0\":hdfs:hdfs:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1939)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1922)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4150)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3075)\n\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3043)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1181)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1177)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1177)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1169)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1924)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:343)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:118)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:484)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:520)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:494)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=aabreu, access=WRITE, inode=\"/data/home/aabreu/airline_data/_temporary/0\":hdfs:hdfs:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1939)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1922)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4150)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy13.mkdirs(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\n\tat com.sun.proxy.$Proxy14.mkdirs(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3073)\n\t... 42 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-17072dfe7ab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m+\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o237.parquet.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=aabreu, access=WRITE, inode=\"/data/home/aabreu/airline_data/_temporary/0\":hdfs:hdfs:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1939)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1922)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4150)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3075)\n\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3043)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1181)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1177)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1177)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1169)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1924)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:343)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:118)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:484)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:520)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:494)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=aabreu, access=WRITE, inode=\"/data/home/aabreu/airline_data/_temporary/0\":hdfs:hdfs:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1939)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1922)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4150)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy13.mkdirs(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\n\tat com.sun.proxy.$Proxy14.mkdirs(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3073)\n\t... 42 more\n"
     ]
    }
   ],
   "source": [
    "# Save the data into my HOME\n",
    "my_home=os.environ['HOME']\n",
    "out_dir=\"airline_data\"\n",
    "df_small.write.partitionBy(\n",
    "        \"Year\",\"Month\"\n",
    "    ).parquet(\n",
    "        my_home\n",
    "        +'/'\n",
    "        + out_dir\n",
    "        ,\n",
    "        mode='overwrite'\n",
    "    )\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input data for year:2014 month:1\n",
      "Reading input data for year:2014 month:2\n",
      "Reading input data for year:2014 month:3\n",
      "Reading input data for year:2014 month:4\n",
      "Reading input data for year:2014 month:5\n",
      "Reading input data for year:2014 month:6\n",
      "Reading input data for year:2014 month:7\n",
      "Reading input data for year:2014 month:8\n",
      "Reading input data for year:2014 month:9\n",
      "Reading input data for year:2014 month:10\n",
      "Reading input data for year:2014 month:11\n",
      "Reading input data for year:2014 month:12\n",
      "Reading input data for year:2015 month:1\n",
      "Reading input data for year:2015 month:2\n",
      "Reading input data for year:2015 month:3\n",
      "Reading input data for year:2015 month:4\n",
      "Reading input data for year:2015 month:5\n",
      "Reading input data for year:2015 month:6\n",
      "Reading input data for year:2015 month:7\n",
      "Reading input data for year:2015 month:8\n",
      "Reading input data for year:2015 month:9\n",
      "Reading input data for year:2015 month:10\n",
      "Reading input data for year:2015 month:11\n",
      "Reading input data for year:2015 month:12\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Read CSV data into a dictionary of DataFrame : try to infer schema directly from the data\n",
    "import itertools\n",
    "year_list = ['2014']\n",
    "month_list = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
    "\n",
    "dict_df = {}\n",
    "\n",
    "for (year_str,month_str) in list(itertools.product(year_list,month_list)):\n",
    "    year_month_str = '%s_%s'%(year_str,month_str)\n",
    "    print('Reading input data for year:%s month:%s'%(year_str,month_str))\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"file://\"+dataset_path+\"On_Time_On_Time_Performance_%s.csv\"%(year_month_str))  \n",
    "    df.cache()\n",
    "    dict_df[year_month_str]=df\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame : inferring Schema Using Databricks library\n",
    "df_with_dbricks = spark.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"On_Time_On_Time_Performance_2014_9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Quarter: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: timestamp (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- AirlineID: integer (nullable = true)\n",
      " |-- Carrier: string (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- OriginAirportID: integer (nullable = true)\n",
      " |-- OriginAirportSeqID: integer (nullable = true)\n",
      " |-- OriginCityMarketID: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCityName: string (nullable = true)\n",
      " |-- OriginState: string (nullable = true)\n",
      " |-- OriginStateFips: integer (nullable = true)\n",
      " |-- OriginStateName: string (nullable = true)\n",
      " |-- OriginWac: integer (nullable = true)\n",
      " |-- DestAirportID: integer (nullable = true)\n",
      " |-- DestAirportSeqID: integer (nullable = true)\n",
      " |-- DestCityMarketID: integer (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCityName: string (nullable = true)\n",
      " |-- DestState: string (nullable = true)\n",
      " |-- DestStateFips: integer (nullable = true)\n",
      " |-- DestStateName: string (nullable = true)\n",
      " |-- DestWac: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepDelayMinutes: double (nullable = true)\n",
      " |-- DepDel15: double (nullable = true)\n",
      " |-- DepartureDelayGroups: integer (nullable = true)\n",
      " |-- DepTimeBlk: string (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- WheelsOff: integer (nullable = true)\n",
      " |-- WheelsOn: integer (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrDelayMinutes: double (nullable = true)\n",
      " |-- ArrDel15: double (nullable = true)\n",
      " |-- ArrivalDelayGroups: integer (nullable = true)\n",
      " |-- ArrTimeBlk: string (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Flights: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- DistanceGroup: integer (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- FirstDepTime: integer (nullable = true)\n",
      " |-- TotalAddGTime: double (nullable = true)\n",
      " |-- LongestAddGTime: double (nullable = true)\n",
      " |-- DivAirportLandings: integer (nullable = true)\n",
      " |-- DivReachedDest: double (nullable = true)\n",
      " |-- DivActualElapsedTime: double (nullable = true)\n",
      " |-- DivArrDelay: double (nullable = true)\n",
      " |-- DivDistance: double (nullable = true)\n",
      " |-- Div1Airport: string (nullable = true)\n",
      " |-- Div1AirportID: integer (nullable = true)\n",
      " |-- Div1AirportSeqID: integer (nullable = true)\n",
      " |-- Div1WheelsOn: integer (nullable = true)\n",
      " |-- Div1TotalGTime: double (nullable = true)\n",
      " |-- Div1LongestGTime: double (nullable = true)\n",
      " |-- Div1WheelsOff: integer (nullable = true)\n",
      " |-- Div1TailNum: string (nullable = true)\n",
      " |-- Div2Airport: string (nullable = true)\n",
      " |-- Div2AirportID: integer (nullable = true)\n",
      " |-- Div2AirportSeqID: integer (nullable = true)\n",
      " |-- Div2WheelsOn: integer (nullable = true)\n",
      " |-- Div2TotalGTime: double (nullable = true)\n",
      " |-- Div2LongestGTime: double (nullable = true)\n",
      " |-- Div2WheelsOff: integer (nullable = true)\n",
      " |-- Div2TailNum: string (nullable = true)\n",
      " |-- Div3Airport: string (nullable = true)\n",
      " |-- Div3AirportID: string (nullable = true)\n",
      " |-- Div3AirportSeqID: string (nullable = true)\n",
      " |-- Div3WheelsOn: string (nullable = true)\n",
      " |-- Div3TotalGTime: string (nullable = true)\n",
      " |-- Div3LongestGTime: string (nullable = true)\n",
      " |-- Div3WheelsOff: string (nullable = true)\n",
      " |-- Div3TailNum: string (nullable = true)\n",
      " |-- Div4Airport: string (nullable = true)\n",
      " |-- Div4AirportID: string (nullable = true)\n",
      " |-- Div4AirportSeqID: string (nullable = true)\n",
      " |-- Div4WheelsOn: string (nullable = true)\n",
      " |-- Div4TotalGTime: string (nullable = true)\n",
      " |-- Div4LongestGTime: string (nullable = true)\n",
      " |-- Div4WheelsOff: string (nullable = true)\n",
      " |-- Div4TailNum: string (nullable = true)\n",
      " |-- Div5Airport: string (nullable = true)\n",
      " |-- Div5AirportID: string (nullable = true)\n",
      " |-- Div5AirportSeqID: string (nullable = true)\n",
      " |-- Div5WheelsOn: string (nullable = true)\n",
      " |-- Div5TotalGTime: string (nullable = true)\n",
      " |-- Div5LongestGTime: string (nullable = true)\n",
      " |-- Div5WheelsOff: string (nullable = true)\n",
      " |-- Div5TailNum: string (nullable = true)\n",
      " |-- _c109: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_dbricks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
