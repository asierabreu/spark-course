{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 : Working with Spark SQL\n",
    "\n",
    "#### We will review :\n",
    "\n",
    "1. Loading CSV file formats using SparkSession\n",
    "2. Creating DataFrame without inferring Schema \n",
    "3. Creating DataFrame inferring Schema \n",
    "4. Doing some preliminary analysis using Spark SQL on this dataset\n",
    "5. Creating UDFs (User Defined Functions) and using them on the dataset\n",
    "5. Saving a DataFrame into partitioned parquet files format\n",
    "\n",
    "#### Small (Lab) Dataset :\n",
    "\n",
    "* Air flight data - subset of ~ 100 MB (for demonstration purposes)\n",
    "* Available in the IE cluster @: /data/shared/spark/flight_data/csv_tiny\n",
    "\n",
    "#### Larger Dataset (Further Labs) :\n",
    "\n",
    "* Air flight data - subset of ~ 2.5 GB (for cluster operation purposes)\n",
    "* Available in the IE cluster @: /data/shared/spark/flight_data/csv_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Let's start by :\n",
    "# 1. Definining SPARK_HOME variable \n",
    "# 2. Using findspark to  let us work with Spark installation in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession and specify configuration\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"Spark-SQL-Lab2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"/data/shared/spark/flight_data/csv_tiny/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all available data files into a data frame\n",
    "df = spark.read \\\n",
    "    .csv(\"file://\"+dataset_path+\"*.csv\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check the data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ok , but the column names are not very telling. \n",
    "* How to improve this? , by telling Spark to use the header ( if exists )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Better , but still one caveat though , all values are interpreted as string, while some of them (actually most), are of numeric nature ( e.g ) Year , Month , Flight Number\n",
    "* How to improve this ?, by either telling Spark what schema to use OR telling it to infer the Schema of the data\n",
    "* Note : Asking Spark to infer schema may have a performance impact depending on the number of rows required to infer the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a table named flights for later SQL queries\n",
    "df.registerTempTable(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the following columns from the full dataset\n",
    "\n",
    "    Year\n",
    "    Month\n",
    "    DayOfMonth\n",
    "    DayOfWeek\n",
    "    FlightNum\n",
    "    Origin\n",
    "    Carrier\n",
    "    Dest ( destination )\n",
    "    DepTime ( departure time )\n",
    "    DepDelayMinutes ( departure delay )\n",
    "    ArrTime ( arrival time )\n",
    "    ArrDelay ( arrival delay )\n",
    "    Cancelled\n",
    "    CancellationCode\n",
    "    AirTime\n",
    "    Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the table for later SQL queries\n",
    "df.registerTempTable(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worth Noting\n",
    "\n",
    "* registerTempTable() creates an in-memory table avaialble within cluster in which it was created. The data is stored using Hive's in-memory columnar format and will only 'live' for the duration of the session.\n",
    "\n",
    "* saveAsTable() creates a permanent, physical table stored using the Parquet format. This table is accessible to all clusters including external clusters and in between sessions. The table metadata including the location of the file(s) is stored within the Hive metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset=spark.sql(\n",
    "    \"select \" \n",
    "    +\"year,month,dayofmonth,dayofweek,\"\n",
    "    +\"flightnum,origin,carrier,dest,deptime,depdelay,\"\n",
    "    +\"arrtime,arrdelay,cancelled,cancellationcode,\"\n",
    "    +\"airtime,distance \"\n",
    "    +\"FROM flights\"\n",
    "    )\n",
    "# OR \n",
    "# selection=[\"year,month,dayofmonth,dayofweek,\"\n",
    "#       \"flightnum,origin,dest,deptime,depdelay,\"\n",
    "#       \"arrtime,arrdelay,cancelled,cancellationcode,\"\n",
    "#       \"airtime,distance \"]\n",
    "# info.select(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache this DataFrame\n",
    "df_subset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 5 rows of the subset data to get a feeling of what to expect\n",
    "df_subset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some SQL queries ( use both the DataFrame and direct SQL queries )\n",
    "\n",
    "1. Find the number of departing flights from a given airport\n",
    "2. Find the total number of delayed flights on a given airport\n",
    "3. Find the average delay per airport\n",
    "4. Find the top 5 airports with the highest average delays\n",
    "5. Find the worst airport in terms of total nb cancelled flights (cancelled=1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many records do we have in total?\n",
    "total=df_subset.count()\n",
    "print('Total nb.of flights: %d' % total)\n",
    "# OR in SQL\n",
    "spark.sql(\"select COUNT(*) from flights\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.2. how many flights and delayed\n",
    "def statsByAirport(airport_id,df):\n",
    "    from_id=df.filter(df['origin']==airport_id)\n",
    "    delayed=from_id.filter(df['depdelay']>=15.0)\n",
    "    ndep=from_id.count()\n",
    "    ndel=delayed.count()\n",
    "    return (ndep,ndel)\n",
    "    \n",
    "airport='JFK'\n",
    "\n",
    "n,m=statsByAirport(airport,df_subset)\n",
    "\n",
    "print('Departing from %s : %d ' %(airport,n))\n",
    "print('Delayed   from %s : %d ' %(airport,m))\n",
    "print('Delayed Percentage : %f %%' %((m/n)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Average delay per flight on an airport\n",
    "def averageDelay(airport_id,df):\n",
    "    from_id=df.filter(df['origin']==airport_id)\n",
    "    return from_id.select('depdelay').describe() # returns a dataframe with descriptive statistics\n",
    "\n",
    "airport='JFK'\n",
    "df=averageDelay(airport,df_subset)\n",
    "\n",
    "print('Airport : %s ' %(airport))\n",
    "print('Average delay : %f min' %(float(df.collect()[1]['depdelay'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Top 5 airports with highest average delay : actually easier here with SQL AVG\n",
    "query = \"SELECT origin,AVG(depdelay) FROM flights GROUP BY origin ORDER BY avg(depdelay) DESC\"\n",
    "df_delays=spark.sql(query)\n",
    "df_delays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. The worst airport in terms of cancelled flights\n",
    "# Create a function that simply sums the total number of flights cancelled on a given airport\n",
    "# ---> Remember you should 'weight' cancelled against total , in order not to bias the result\n",
    "#\n",
    "def cancellations(airport_id):\n",
    "    # USING DF: \n",
    "    #     idf=df_subset.filter(df_subset['origin']==airport_id)\n",
    "    #     return idf.filter(idf['cancelled']==1.0).count()\n",
    "    # USING SQL:\n",
    "    query=\"select * from flights where origin=='\"+airport_id+\"'\" +\" and cancelled==1.0\"\n",
    "    return spark.sql(query).select('cancelled').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the function with Spark SQL as User Defined Function\n",
    "spark.udf.register(\"cancellations\", lambda x : cancellations(x))\n",
    "query = \"SELECT origin,cancellations(origin) AS score FROM flights GROUP BY origin ORDER BY score DESC\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save this dataframe in parquet (columnar) format for boost in loading performance\n",
    "* In order to do we want to 'be clever' and partition the data by specific atributes , in this case\n",
    "* Year and Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data into my HOME\n",
    "# IMPORTANT NOTE: we are partinioning (structuring)\n",
    "# by relevant factors in our data , in this case year and month\n",
    "# can be used to naturally save this data.\n",
    "my_home=os.environ['HOME']\n",
    "out_dir=\"airline_data\"\n",
    "df_subset.write.partitionBy(\n",
    "        \"Year\",\"Month\"\n",
    "    ).parquet(\n",
    "        \"file://\"\n",
    "        + my_home\n",
    "        +'/'\n",
    "        + out_dir,\n",
    "        mode='overwrite'\n",
    "    )\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV data into a dictionary of DataFrame : try to infer schema directly from the data\n",
    "import itertools\n",
    "year_list = ['2014']\n",
    "month_list = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
    "\n",
    "dict_df = {}\n",
    "\n",
    "for (year_str,month_str) in list(itertools.product(year_list,month_list)):\n",
    "    year_month_str = '%s_%s'%(year_str,month_str)\n",
    "    print('Reading input data for year:%s month:%s'%(year_str,month_str))\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"file://\"+dataset_path+\"On_Time_On_Time_Performance_%s.csv\"%(year_month_str))  \n",
    "    df.cache()\n",
    "    dict_df[year_month_str]=df\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
