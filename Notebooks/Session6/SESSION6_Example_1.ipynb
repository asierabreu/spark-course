{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SESSION 6 : Example 1\n",
    "\n",
    "### Topics : \n",
    "\n",
    "* RDD Creation\n",
    "* RDD Partioning \n",
    "* RDD Operations and Shuffling\n",
    "* RDD Persistence\n",
    "\n",
    "### Example objetive :\n",
    "\n",
    "Given a list of credit card payments per customer , compute:\n",
    "\n",
    " 1. The total payments per country\n",
    " 2. The avg amount per country\n",
    " 3. The min and max payment across all countries\n",
    "\n",
    "### Reference :\n",
    "\n",
    "* SPARK Reference Documentation: https://spark.apache.org/docs/2.1.1/programming-guide.html#rdd-operations\n",
    "\n",
    "### Additional Info :\n",
    "\n",
    "* RDD Partitions : https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-partitions.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3.6\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SparkSession and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Session6-Example-1\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "# context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version  : 2.3.2\n",
      "Spark app ID   : local-1542464841172\n",
      "Spark app name : PySparkShell\n",
      "Spark mode     : local[*]\n"
     ]
    }
   ],
   "source": [
    "# Configuration Check\n",
    "print(\"Spark version  : \"+str(sc.version))\n",
    "print(\"Spark app ID   : \"+sc.applicationId)\n",
    "print(\"Spark app name : \"+sc.appName)\n",
    "print(\"Spark mode     : \"+sc.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer credit card payment records , dummy list\n",
    "payments= [\n",
    "    {'name': 'customer01', 'amount': 500, 'country': 'India'},\n",
    "    {'name': 'customer02', 'amount': 150, 'country': 'India'},\n",
    "    {'name': 'customer03', 'amount': 50 , 'country': 'India'},\n",
    "    {'name': 'customer04', 'amount': 200, 'country': 'Germany'},\n",
    "    {'name': 'customer05', 'amount': 750, 'country': 'India'},\n",
    "    {'name': 'customer06', 'amount': 100, 'country': 'Poland'},\n",
    "    {'name': 'customer08', 'amount': 100, 'country': 'Poland'},\n",
    "    {'name': 'customer08', 'amount': 100, 'country': 'Spain'},\n",
    "    {'name': 'customer09', 'amount': 100, 'country': 'Spain'},\n",
    "    {'name': 'customer10', 'amount': 200, 'country': 'Spain'},\n",
    "    {'name': 'customer11', 'amount': 100, 'country': 'Spain'},\n",
    "    {'name': 'customer12', 'amount': 100, 'country': 'Spain'},\n",
    "    {'name': 'customer13', 'amount': 100, 'country': 'Germany'},\n",
    "    {'name': 'customer14', 'amount': 300, 'country': 'Germany'},\n",
    "    {'name': 'customer15', 'amount': 100, 'country': 'Germany'},\n",
    "    {'name': 'customer16', 'amount': 100, 'country': 'Spain'},\n",
    "    {'name': 'customer17', 'amount': 100, 'country': 'Poland'},\n",
    "    {'name': 'customer18', 'amount': 400, 'country': 'India'},\n",
    "    {'name': 'customer19', 'amount': 100, 'country': 'India'},\n",
    "    {'name': 'customer20', 'amount': 100, 'country': 'India'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an RDD : by parallelizing a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and RDD by parallelizing this list\n",
    "# specify the nb of partitions\n",
    "nb_of_partitions=4\n",
    "rdd = sc.parallelize(payments,nb_of_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory : each paritition will have one task associated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/partitions.png\" height=\"700px\" width=\"700px\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Details\n",
    " \n",
    " *  For PairwiseRDDs (key,value) data is distributed between partitions depending on the value of the key.\n",
    " *  For the rest data is simply distributed into chunks containing consecutive records according to either the nb. of records or size of a chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Partition Details\n",
    "\n",
    " * Get RDD partition numbers : with getNumPartitions() function \n",
    " * Inspect RDD partition contents : with glom() function\n",
    " \n",
    " * **Spark partitioner** (see API : https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/Partitioner.html) \n",
    "   * Partitioner defines how the elements in a key-value pair RDD are partitioned by key. \n",
    "   * Maps each key to a partition ID, from 0  to numPartitions - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parallelism : 2\n",
      "Number of partitions: 4\n",
      "Partitioner         : None\n",
      "Partitions structure: \n",
      "\n",
      "partition 1 : [{'name': 'customer01', 'amount': 500, 'country': 'India'}, {'name': 'customer02', 'amount': 150, 'country': 'India'}, {'name': 'customer03', 'amount': 50, 'country': 'India'}, {'name': 'customer04', 'amount': 200, 'country': 'Germany'}, {'name': 'customer05', 'amount': 750, 'country': 'India'}]\n",
      "nb of elements: 5\n",
      "\n",
      "partition 2 : [{'name': 'customer06', 'amount': 100, 'country': 'Poland'}, {'name': 'customer08', 'amount': 100, 'country': 'Poland'}, {'name': 'customer08', 'amount': 100, 'country': 'Spain'}, {'name': 'customer09', 'amount': 100, 'country': 'Spain'}, {'name': 'customer10', 'amount': 200, 'country': 'Spain'}]\n",
      "nb of elements: 5\n",
      "\n",
      "partition 3 : [{'name': 'customer11', 'amount': 100, 'country': 'Spain'}, {'name': 'customer12', 'amount': 100, 'country': 'Spain'}, {'name': 'customer13', 'amount': 100, 'country': 'Germany'}, {'name': 'customer14', 'amount': 300, 'country': 'Germany'}, {'name': 'customer15', 'amount': 100, 'country': 'Germany'}]\n",
      "nb of elements: 5\n",
      "\n",
      "partition 4 : [{'name': 'customer16', 'amount': 100, 'country': 'Spain'}, {'name': 'customer17', 'amount': 100, 'country': 'Poland'}, {'name': 'customer18', 'amount': 400, 'country': 'India'}, {'name': 'customer19', 'amount': 100, 'country': 'India'}, {'name': 'customer20', 'amount': 100, 'country': 'India'}]\n",
      "nb of elements: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Default parallelism : {}\".format(sc.defaultParallelism))\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner         : {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: \")\n",
    "# Note that collect() is an action\n",
    "i=1\n",
    "for partition in rdd.glom().collect():\n",
    "    print('')\n",
    "    print('partition %d : %s' %(i,partition))\n",
    "    print('nb of elements: %d'   %len(partition))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition contents \n",
    "What we can see is that Spark has evenly distributed the data across the different partitions ( 5 elements on each of the 4 partitions).\n",
    "This even distribution is performed because the data is simply a collection of items , not a key,value list of items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inspecting Spark Jobs\n",
    "* What is the expected execution behavior , how many tasks?\n",
    "* Spark Application can be inspected  @ Spark History Server  http://localhost:18081/ <br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply some transformation + action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total of payments per country\n",
    "# \n",
    "def sum_payments(group):\n",
    "    # ---------------\n",
    "    # TODO : exercise\n",
    "    # ---------------\n",
    "    return  \n",
    "\n",
    "# To group all customers by country\n",
    "# Apply a 2 transformations (groupBy and map) and an action (collect)\n",
    "# Tip : the map transformation is required here because the result of the groupBy transformation returns\n",
    "# an iterable object, and we actually want to transform it into a list\n",
    "result1 = rdd \\\n",
    "    .groupBy(lambda customer: customer['country']) \\\n",
    "    .map(lambda group : (group[0], list(group[1]))) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Poland',\n",
       "  [{'name': 'customer06', 'amount': 100, 'country': 'Poland'},\n",
       "   {'name': 'customer08', 'amount': 100, 'country': 'Poland'},\n",
       "   {'name': 'customer17', 'amount': 100, 'country': 'Poland'}]),\n",
       " ('Germany',\n",
       "  [{'name': 'customer04', 'amount': 200, 'country': 'Germany'},\n",
       "   {'name': 'customer13', 'amount': 100, 'country': 'Germany'},\n",
       "   {'name': 'customer14', 'amount': 300, 'country': 'Germany'},\n",
       "   {'name': 'customer15', 'amount': 100, 'country': 'Germany'}]),\n",
       " ('Spain',\n",
       "  [{'name': 'customer08', 'amount': 100, 'country': 'Spain'},\n",
       "   {'name': 'customer09', 'amount': 100, 'country': 'Spain'},\n",
       "   {'name': 'customer10', 'amount': 200, 'country': 'Spain'},\n",
       "   {'name': 'customer11', 'amount': 100, 'country': 'Spain'},\n",
       "   {'name': 'customer12', 'amount': 100, 'country': 'Spain'},\n",
       "   {'name': 'customer16', 'amount': 100, 'country': 'Spain'}]),\n",
       " ('India',\n",
       "  [{'name': 'customer01', 'amount': 500, 'country': 'India'},\n",
       "   {'name': 'customer02', 'amount': 150, 'country': 'India'},\n",
       "   {'name': 'customer03', 'amount': 50, 'country': 'India'},\n",
       "   {'name': 'customer05', 'amount': 750, 'country': 'India'},\n",
       "   {'name': 'customer18', 'amount': 400, 'country': 'India'},\n",
       "   {'name': 'customer19', 'amount': 100, 'country': 'India'},\n",
       "   {'name': 'customer20', 'amount': 100, 'country': 'India'}])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inspecting Spark Jobs\n",
    "* What is the expected execution behavior , was there a shuffle , why?\n",
    "* Spark Application can be inspected  @ Spark History Server  http://34.251.237.234:18081/ <br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could we reducing the shuffling?\n",
    "* Yes, Using a custom partitioner : HashPartitioning\n",
    "* **Note** : custom partioning is only possible for key,value PairRDD\n",
    "* Let's define a custom partitioner that will allows us to have all customer for a given country in one partition\n",
    "* We use a hash function which returns a unique integer number for the given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom partitioner based on the hash (python function) of a String\n",
    "# this python function returns a unique integer number for a given string\n",
    "def country_partitioner(country):\n",
    "    return hash(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new RDD but now using our defined partitioner\n",
    "# --------------\n",
    "# Important Note : custom partioning can only be applied to (K,V) pairRDDs \n",
    "#                  therefore we need to create key,value pair RDD below  \n",
    "# --------------\n",
    "new_rdd = sc.parallelize(payments) \\\n",
    "    .map(lambda customer: (customer['country'],customer)) \\\n",
    "    .partitionBy(4, country_partitioner) \\\n",
    "    .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total of payments per country\n",
    "# \n",
    "\n",
    "# By using previously the partitioner we placed all customers for a given country in a partition\n",
    "# hence improving data locality for our required operation and avoiding shuffling\n",
    "\n",
    "# define the summing function\n",
    "def sum_payments(iterator):\n",
    "    yield sum(group[1]['amount'] for group in iterator)\n",
    "\n",
    "\n",
    "# Now we simply apply a function to each partition with the mapPartitions() transformation\n",
    "# Therefore this transformation is done in one node with NO SHUFFLING\n",
    "amount_per_country = new_rdd \\\n",
    "        .mapPartitions(sum_payments) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300, 1400, 2050, 0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_per_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parallelism : 2\n",
      "Number of partitions: 4\n",
      "Partitioner         : None\n",
      "Partitions structure: \n",
      "\n",
      "partition 1 : [{'name': 'customer01', 'amount': 500, 'country': 'India'}, {'name': 'customer02', 'amount': 150, 'country': 'India'}, {'name': 'customer03', 'amount': 50, 'country': 'India'}, {'name': 'customer04', 'amount': 200, 'country': 'Germany'}, {'name': 'customer05', 'amount': 750, 'country': 'India'}]\n",
      "nb of elements: 5\n",
      "\n",
      "partition 2 : [{'name': 'customer06', 'amount': 100, 'country': 'Poland'}, {'name': 'customer08', 'amount': 100, 'country': 'Poland'}, {'name': 'customer08', 'amount': 100, 'country': 'Spain'}, {'name': 'customer09', 'amount': 100, 'country': 'Spain'}, {'name': 'customer10', 'amount': 200, 'country': 'Spain'}]\n",
      "nb of elements: 5\n",
      "\n",
      "partition 3 : [{'name': 'customer11', 'amount': 100, 'country': 'Spain'}, {'name': 'customer12', 'amount': 100, 'country': 'Spain'}, {'name': 'customer13', 'amount': 100, 'country': 'Germany'}, {'name': 'customer14', 'amount': 300, 'country': 'Germany'}, {'name': 'customer15', 'amount': 100, 'country': 'Germany'}]\n",
      "nb of elements: 5\n",
      "\n",
      "partition 4 : [{'name': 'customer16', 'amount': 100, 'country': 'Spain'}, {'name': 'customer17', 'amount': 100, 'country': 'Poland'}, {'name': 'customer18', 'amount': 400, 'country': 'India'}, {'name': 'customer19', 'amount': 100, 'country': 'India'}, {'name': 'customer20', 'amount': 100, 'country': 'India'}]\n",
      "nb of elements: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Default parallelism : {}\".format(sc.defaultParallelism))\n",
    "print(\"Number of partitions: {}\".format(new_rdd.getNumPartitions()))\n",
    "print(\"Partitioner         : {}\".format(new_rdd.partitioner))\n",
    "print(\"Partitions structure: \")\n",
    "# Note that collect() is an action\n",
    "i=1\n",
    "for partition in new_rdd.glom().collect():\n",
    "    print('')\n",
    "    print('partition %d : %s' %(i,partition))\n",
    "    print('nb of elements: %d'   %len(partition))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
