{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SESSION 6 : Example 2\n",
    "\n",
    "### Topics : \n",
    "\n",
    "* Dataframe Creation\n",
    "* Dataframe and SQL Operations\n",
    "* Dataframe Persistence\n",
    "\n",
    "### Objetive :\n",
    "\n",
    " * Getting familiar with Spark Dataframes programming and SQL operations.\n",
    " * Get to know the different formats used in big data\n",
    " * Intro to the dataframes persistence\n",
    "\n",
    "### Reference :\n",
    "\n",
    "* SPARK Reference Documentation: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "### Additional Info :\n",
    "\n",
    "* Data sources and formats : https://spark.apache.org/docs/latest/sql-data-sources.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3.6\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SparkSession and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Session6-Example-2\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "# context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version  : 2.3.2\n",
      "Spark app ID   : local-1542571339786\n",
      "Spark app name : PySparkShell\n",
      "Spark mode     : local[*]\n"
     ]
    }
   ],
   "source": [
    "# Configuration Check\n",
    "print(\"Spark version  : \"+str(sc.version))\n",
    "print(\"Spark app ID   : \"+sc.applicationId)\n",
    "print(\"Spark app name : \"+sc.appName)\n",
    "print(\"Spark mode     : \"+sc.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer credit card payment records , dummy list of dictionaries\n",
    "payments= [\n",
    "    {'name': 'customer01', 'amount': 500, 'country': 'India','age':20},\n",
    "    {'name': 'customer02', 'amount': 150, 'country': 'India','age':25},\n",
    "    {'name': 'customer03', 'amount': 50 , 'country': 'India','age':30},\n",
    "    {'name': 'customer04', 'amount': 200, 'country': 'Germany','age':35},\n",
    "    {'name': 'customer05', 'amount': 750, 'country': 'India','age':20},\n",
    "    {'name': 'customer06', 'amount': 100, 'country': 'Poland','age':35},\n",
    "    {'name': 'customer08', 'amount': 100, 'country': 'Poland','age':45},\n",
    "    {'name': 'customer08', 'amount': 100, 'country': 'Spain','age':300},\n",
    "    {'name': 'customer09', 'amount': 100, 'country': 'Spain','age':50},\n",
    "    {'name': 'customer10', 'amount': 200, 'country': 'Spain','age':60},\n",
    "    {'name': 'customer11', 'amount': 100, 'country': 'Spain','age':25},\n",
    "    {'name': 'customer12', 'amount': 100, 'country': 'Spain','age':2},\n",
    "    {'name': 'customer13', 'amount': 100, 'country': 'Germany','age':50},\n",
    "    {'name': 'customer14', 'amount': 300, 'country': 'Germany','age':40},\n",
    "    {'name': 'customer15', 'amount': 100, 'country': 'Germany','age':35},\n",
    "    {'name': 'customer16', 'amount': 100, 'country': 'Spain','age':25},\n",
    "    {'name': 'customer17', 'amount': 100, 'country': 'Poland','age':45},\n",
    "    {'name': 'customer18', 'amount': 400, 'country': 'India','age':50},\n",
    "    {'name': 'customer19', 'amount': 100, 'country': 'India','age':50},\n",
    "    {'name': 'customer20', 'amount': 100, 'country': 'India','age':20},\n",
    "    {'name': 'customer21', 'amount': 500, 'country': 'India','age':20},\n",
    "    {'name': 'customer22', 'amount': 150, 'country': 'India','age':20},\n",
    "    {'name': 'customer23', 'amount': 50 , 'country': 'India','age':30},\n",
    "    {'name': 'customer24', 'amount': 200, 'country': 'Germany','age':60},\n",
    "    {'name': 'customer25', 'amount': 750, 'country': 'India','age':20},\n",
    "    {'name': 'customer26', 'amount': 100, 'country': 'Poland','age':20},\n",
    "    {'name': 'customer28', 'amount': 100, 'country': 'Poland','age':20},\n",
    "    {'name': 'customer28', 'amount': 100, 'country': 'Spain','age':30},\n",
    "    {'name': 'customer29', 'amount': 300, 'country': 'Spain','age':20},\n",
    "    {'name': 'customer30', 'amount': 200, 'country': 'Spain','age':50},\n",
    "    {'name': 'customer31', 'amount': 100, 'country': 'Spain','age':40},\n",
    "    {'name': 'customer32', 'amount': 100, 'country': 'Spain','age':40},\n",
    "    {'name': 'customer33', 'amount': 100, 'country': 'Germany','age':40},\n",
    "    {'name': 'customer34', 'amount': 300, 'country': 'Germany','age':20},\n",
    "    {'name': 'customer35', 'amount': 100, 'country': 'Germany','age':40},\n",
    "    {'name': 'customer36', 'amount': 100, 'country': 'Spain','age':50},\n",
    "    {'name': 'customer37', 'amount': 100, 'country': 'Poland','age':30},\n",
    "    {'name': 'customer38', 'amount': 400, 'country': 'India','age':20},\n",
    "    {'name': 'customer39', 'amount': 100, 'country': 'India','age':20},\n",
    "    {'name': 'customer40', 'amount': 100, 'country': 'India','age':30},\n",
    "    {'name': 'customer41', 'amount': 500, 'country': 'India','age':35},\n",
    "    {'name': 'customer42', 'amount': 150, 'country': 'India','age':35},\n",
    "    {'name': 'customer43', 'amount': 50 , 'country': 'India','age':40},\n",
    "    {'name': 'customer44', 'amount': 200, 'country': 'Germany','age':20},\n",
    "    {'name': 'customer45', 'amount': 750, 'country': 'India','age':20},\n",
    "    {'name': 'customer46', 'amount': 100, 'country': 'Poland','age':20},\n",
    "    {'name': 'customer48', 'amount': 100, 'country': 'Poland','age':20},\n",
    "    {'name': 'customer48', 'amount': 100, 'country': 'Spain','age':30},\n",
    "    {'name': 'customer49', 'amount': 800, 'country': 'Spain','age':50},\n",
    "    {'name': 'customer50', 'amount': 200, 'country': 'Spain','age':20},\n",
    "    {'name': 'customer51', 'amount': 100, 'country': 'Spain','age':50},\n",
    "    {'name': 'customer52', 'amount': 100, 'country': 'Spain','age':30},\n",
    "    {'name': 'customer53', 'amount': 100, 'country': 'Germany','age':40},\n",
    "    {'name': 'customer54', 'amount': 300, 'country': 'Germany','age':40},\n",
    "    {'name': 'customer55', 'amount': 100, 'country': 'Germany','age':40},\n",
    "    {'name': 'customer56', 'amount': 100, 'country': 'Spain','age':20},\n",
    "    {'name': 'customer57', 'amount': 100, 'country': 'Poland','age':20},\n",
    "    {'name': 'customer58', 'amount': 400, 'country': 'India','age':20},\n",
    "    {'name': 'customer59', 'amount': 100, 'country': 'India','age':25},\n",
    "    {'name': 'customer60', 'amount': 100, 'country': 'India','age':25},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dataframe from and RDD:\n",
    "\n",
    "Steps :\n",
    "1. Creating the RDD \n",
    "2. Define the schema required for the Dataframe or infer it from the data\n",
    "3. Create the actual Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and RDD by parallelizing this list\n",
    "rdd = sc.parallelize(payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/software/spark/python/pyspark/sql/session.py:360: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "# in this particular case we could even infer the schema from the RDD itself\n",
    "# because we are using dictionaries\n",
    "df = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Remember the notion of schema?\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+----------+\n",
      "|age|amount|country|      name|\n",
      "+---+------+-------+----------+\n",
      "| 20|   500|  India|customer01|\n",
      "| 25|   150|  India|customer02|\n",
      "| 30|    50|  India|customer03|\n",
      "| 35|   200|Germany|customer04|\n",
      "| 20|   750|  India|customer05|\n",
      "| 35|   100| Poland|customer06|\n",
      "| 45|   100| Poland|customer08|\n",
      "|300|   100|  Spain|customer08|\n",
      "| 50|   100|  Spain|customer09|\n",
      "| 60|   200|  Spain|customer10|\n",
      "| 25|   100|  Spain|customer11|\n",
      "|  2|   100|  Spain|customer12|\n",
      "| 50|   100|Germany|customer13|\n",
      "| 40|   300|Germany|customer14|\n",
      "| 35|   100|Germany|customer15|\n",
      "| 25|   100|  Spain|customer16|\n",
      "| 45|   100| Poland|customer17|\n",
      "| 50|   400|  India|customer18|\n",
      "| 50|   100|  India|customer19|\n",
      "| 20|   100|  India|customer20|\n",
      "+---+------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's inspect the contents\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe operations by example :\n",
    "\n",
    "Compute the following :\n",
    "\n",
    " 1. The number of customer per country\n",
    " 2. The total payments per country\n",
    " 3. The avg amount per country\n",
    " 4. The min and max payment across all countries\n",
    " \n",
    "FOR YOU Compute the following :\n",
    "\n",
    " 5. The average age of our customers per country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|country|count|\n",
      "+-------+-----+\n",
      "|Germany|   12|\n",
      "|  India|   21|\n",
      "|  Spain|   18|\n",
      "| Poland|    9|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. nb of customers per country\n",
    "df.groupBy('country').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|country|sum(amount)|\n",
      "+-------+-----------+\n",
      "|Germany|       2100|\n",
      "|  India|       6150|\n",
      "|  Spain|       3000|\n",
      "| Poland|        900|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. total payments per country\n",
    "import pyspark.sql.functions as F\n",
    "df.groupBy('country').agg(F.sum('amount')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|country|       avg(amount)|\n",
      "+-------+------------------+\n",
      "|Germany|             175.0|\n",
      "|  India|292.85714285714283|\n",
      "|  Spain|166.66666666666666|\n",
      "| Poland|             100.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. avg amount per country\n",
    "import pyspark.sql.functions as F\n",
    "df.groupBy('country').agg(F.avg('amount')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(amount)|\n",
      "+-----------+\n",
      "|        800|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. min/max for all countries\n",
    "df.agg(F.max('amount')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(amount)|\n",
      "+-----------+\n",
      "|         50|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.min('amount')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe SQL operations by example :\n",
    "\n",
    "Compute the same quantities using spark SQL API  :\n",
    "\n",
    " 1. The number of customer per country\n",
    " 2. The total payments per country\n",
    " 3. The avg amount per country\n",
    " 4. The min and max payment across all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First we create a temporary table in the Hive metastore db\n",
    "df.registerTempTable(\"payments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"SELECT * FROM payments\"\n",
    "sql_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|amount|country|      name|\n",
      "+------+-------+----------+\n",
      "|   500|  India|customer01|\n",
      "|   150|  India|customer02|\n",
      "|    50|  India|customer03|\n",
      "|   200|Germany|customer04|\n",
      "|   750|  India|customer05|\n",
      "|   100| Poland|customer06|\n",
      "|   100| Poland|customer08|\n",
      "|   100|  Spain|customer08|\n",
      "|   100|  Spain|customer09|\n",
      "|   200|  Spain|customer10|\n",
      "|   100|  Spain|customer11|\n",
      "|   100|  Spain|customer12|\n",
      "|   100|Germany|customer13|\n",
      "|   300|Germany|customer14|\n",
      "|   100|Germany|customer15|\n",
      "|   100|  Spain|customer16|\n",
      "|   100| Poland|customer17|\n",
      "|   400|  India|customer18|\n",
      "|   100|  India|customer19|\n",
      "|   100|  India|customer20|\n",
      "+------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|country|count|\n",
      "+-------+-----+\n",
      "|Germany|   12|\n",
      "|  India|   21|\n",
      "|  Spain|   18|\n",
      "| Poland|    9|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. The number of customer per country in SQL\n",
    "query=\"SELECT country,COUNT(*) as count FROM payments GROUP BY country\" \n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|country|total|\n",
      "+-------+-----+\n",
      "|Germany| 2100|\n",
      "|  India| 6150|\n",
      "|  Spain| 3000|\n",
      "| Poland|  900|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. The total payments per country\n",
    "query=\"SELECT country,SUM(amount) as total FROM payments GROUP BY country\" \n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|country|             total|\n",
      "+-------+------------------+\n",
      "|Germany|             175.0|\n",
      "|  India|292.85714285714283|\n",
      "|  Spain|166.66666666666666|\n",
      "| Poland|             100.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. The avg amount per country\n",
    "query=\"SELECT country,AVG(amount) as total FROM payments GROUP BY country\" \n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|max|\n",
      "+---+\n",
      "|800|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. The min and max payment across all countries\n",
    "query=\"SELECT MAX(amount) as max FROM payments\" \n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|min|\n",
      "+---+\n",
      "| 50|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. The min and max payment across all countries\n",
    "query=\"SELECT MIN(amount) as min FROM payments\" \n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting Dataframes\n",
    "* We see an example of persisting a dataframe using a big data format parquet\n",
    "* We also see that we are partitioning the output by specific 'attributes' of the data\n",
    "* We do this generally to improve the performance of later data loading and querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original dataframe \n",
    "# partitioning the output by country and age into parquet format.\n",
    "my_home=os.environ['HOME']\n",
    "out_dir=\"spark_sessions/session6/data/\"\n",
    "df.write.partitionBy(\n",
    "        \"age\",\"country\"\n",
    "    ).parquet(\n",
    "        \"file://\"\n",
    "        + my_home\n",
    "        +'/'\n",
    "        + out_dir,\n",
    "        mode='overwrite'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\r\n",
      "drwxrwxr-x 3 ubuntu ubuntu 4096 Nov 18 21:06 age=2\r\n",
      "drwxrwxr-x 6 ubuntu ubuntu 4096 Nov 18 21:06 age=20\r\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Nov 18 21:06 age=25\r\n",
      "drwxrwxr-x 5 ubuntu ubuntu 4096 Nov 18 21:06 age=30\r\n",
      "drwxrwxr-x 3 ubuntu ubuntu 4096 Nov 18 21:06 age=300\r\n",
      "drwxrwxr-x 5 ubuntu ubuntu 4096 Nov 18 21:06 age=35\r\n",
      "drwxrwxr-x 5 ubuntu ubuntu 4096 Nov 18 21:06 age=40\r\n",
      "drwxrwxr-x 3 ubuntu ubuntu 4096 Nov 18 21:06 age=45\r\n",
      "drwxrwxr-x 5 ubuntu ubuntu 4096 Nov 18 21:06 age=50\r\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Nov 18 21:06 age=60\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu    0 Nov 18 21:06 _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "### You can inspect the contents of what you just saved\n",
    "! ls -l $my_home/$out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\r\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4096 Nov 18 21:06 country=Germany\r\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4096 Nov 18 21:06 country=India\r\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4096 Nov 18 21:06 country=Spain\r\n"
     ]
    }
   ],
   "source": [
    "# You can see how the output data files have been structured according to the requested partitoning scheme\n",
    "# age + country\n",
    "! ls -l $my_home/$out_dir/age=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 629 Nov 18 21:06 part-00000-64f3c1b1-3ad9-4ada-9b84-f4c996f9d673.c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 661 Nov 18 21:06 part-00001-64f3c1b1-3ad9-4ada-9b84-f4c996f9d673.c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l $my_home/$out_dir/age=50/country=Spain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
